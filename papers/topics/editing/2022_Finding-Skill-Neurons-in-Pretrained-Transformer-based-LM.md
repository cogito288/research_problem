📌TL;DR: ****



## Problem 
* **👀 Motivation**: It remains unclear how the skills required to handle NLP tasks distribute among model parameters

* **❓ General Problem**: Are there specific neurons within pre-trained Transformers encoding task-specific skills?

* **✅ Solved**: Develop a simple and effective method to find them for classification tasks via **prompt tuning**.
    - test

* **🤔 Unsolved (Limitations)**:

* **💡 New Problem** :

* **🌹 if the proposed algorithm fundamentally solves the given problem and if it does, think about which aspect of the algorithm makes the problem solved**

# Paper

### Paper Info 
* Title : Finding Skill Neurons in Pre-trained Transformer-based Language Models
* Authors: Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, Juanzi Li
* Publication : 2022.11.14
* paper link : https://arxiv.org/pdf/2211.07349.pdf

### Page Info 
* Contributors: Youngju Joung
* 2023.05.11

## Summary 
<p align="center"><img src="../../figures/.png" width="750" height="300">


## Discussion